{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ce30fb69-6e20-4a41-91fe-a4f5a72cbbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from templates import prompt_slot_filling\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "279df358-c17f-4425-beaa-0ba98b284378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'YOUR_API_KEY' with your actual API key from OpenAI\n",
    "api_key = \"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\"\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b50ed44-5ecf-41ba-a6cc-7a7f5d5df5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rhet_data_path = '../data/slots_data/rhet_data.json'\n",
    "slot_path = '../data/slots_data/rhet_data_slots.json'\n",
    "\n",
    "task_objects = []\n",
    "with open(rhet_data_path, 'r') as f:\n",
    "    rhet_data = json.load(f)\n",
    "    for file in rhet_data:\n",
    "        rdata = rhet_data[file]\n",
    "        for section, text in rdata.items():\n",
    "            section_file = os.path.join('../data/slots_data',section+'.txt')\n",
    "            with open(section_file, 'r') as f1:\n",
    "                section_slots = f1.read().strip()\n",
    "            task_objects.append(\n",
    "                {\n",
    "                    \"file\": file,\n",
    "                    \"section_slots\": section_slots,\n",
    "                    \"section\": section,\n",
    "                    \"text\": text\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de9fdf08-da0c-43bd-b99b-d1ca20df59b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    }
   ],
   "source": [
    "print(len(task_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c27723c8-fcc2-41d3-85f9-10553c9a8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(messages, model):\n",
    "    # Set the model and the prompt for the conversation\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        # Generate a chat response from the model\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    elif model == \"gpt-3.5-turbo-instruct\":\n",
    "        # Generate a chat response from the model\n",
    "        response = openai.Completion.create(\n",
    "            model=model,\n",
    "            prompt=messages,\n",
    "            temperature=0,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        all_responses = []\n",
    "        for ind in range(0, len(response['choices'])):\n",
    "            all_responses.append(response['choices'][ind]['text'])\n",
    "        return all_responses\n",
    "    \n",
    "    else:\n",
    "        assert \"Model not in supported model list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8052209f-9b3f-4773-b1ec-b5bedcea1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slot_txt_to_json(text):\n",
    "    text = text.strip()\n",
    "    text = text.split('\\n')\n",
    "    resp_dict = {}\n",
    "    for line in text:\n",
    "        sep_ind = line.find(\":\")\n",
    "        if sep_ind ==-1:\n",
    "            assert f\"seperator error for sep line {line}\"\n",
    "        key = line[:sep_ind].strip()\n",
    "        value = line[sep_ind+1:].strip()\n",
    "        resp_dict[key]=value\n",
    "        \n",
    "    return resp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d52556fd-0c5c-4789-bfee-9c2d0086601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhet_data_copy = copy.deepcopy(rhet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d4728e6-ac06-408d-a5d7-fad8275fb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_indices(bs, total_batches, total_length):\n",
    "    batch_begin = 0\n",
    "    batch_end = 0\n",
    "    for i in range(total_batches):\n",
    "        batch_begin=batch_end\n",
    "        batch_end=min(batch_end+bs, total_length)\n",
    "        yield (batch_begin, batch_end)\n",
    "        \n",
    "bs = 4\n",
    "total_batches = len(task_objects)//bs\n",
    "total_length = len(task_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8183edc5-b710-4f6c-8d66-4b16dd0735e3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 900/900 [42:05<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_ind = batch_indices(bs=bs, total_batches=total_batches, total_length=total_length)\n",
    "\n",
    "for i in tqdm(range(total_batches), total=total_batches):\n",
    "    b_ind = next(batch_ind)\n",
    "    task_batch = task_objects[b_ind[0]: b_ind[1]]\n",
    "    \n",
    "    openai_prompts = []\n",
    "    for task in task_batch:\n",
    "        text = task['text']\n",
    "        slots = task['section_slots']\n",
    "        file = task['file']\n",
    "        section = task['section']\n",
    "        openai_prompt = prompt_slot_filling.substitute(text=text, slots=slots)\n",
    "        openai_prompts.append(openai_prompt)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        resps = generate_response(messages = openai_prompts,\n",
    "                                 model = 'gpt-3.5-turbo-instruct')\n",
    "    except Exception as e:\n",
    "        print(f'Error at i {i}, batch start: {b_ind[0]}, batch end: {b_ind[1]}')\n",
    "        print(f'openai prompt: ', openai_prompts)\n",
    "        print('Skipping..\\n')\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # populate the batch results\n",
    "    for task, resp in zip(task_batch, resps):\n",
    "        text = task['text']\n",
    "        file = task['file']\n",
    "        section = task['section']\n",
    "        rhet_data_copy[file][section] = {\n",
    "            \"text\": text,\n",
    "            \"slots\": slot_txt_to_json(resp)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9b259d9b-a67f-4b60-8c82-77bebcd5b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(slot_path, 'w') as f:\n",
    "    json.dump(rhet_data_copy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264baa64-d0ad-453f-97a9-dbdb67c9fd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_env1",
   "language": "python",
   "name": "all_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
